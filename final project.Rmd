---
title: "Machine Learning Project"
author: "Dan Taylor Lewis"
date: "26/07/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Download and save data

```{r,echo=FALSE}
download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",destfile = "pml_training.csv")
download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",destfile="pml_testing.csv")

```

# load required libraries

```{r}
library(dplyr)
library(caret)
```



# Read in and inspect data
## Read data
During the data inspection discover the NA strings "NA" "#DIV/0!" added this to the read.csv function so variable with this string can be read in without error.

```{r}

training<-read.csv("pml_training.csv",header =T,na.strings = c("","#DIV/0!","NA"))
testing<-read.csv("pml_testing.csv",header = T,na.strings = c("","#DIV/0!","NA"))

```
## Inspect and clean data

I have not echoed the below code chunk because I have added fixes for errors flagged in the read data step and the output is also quite lengthy and is not important to this project. 

```{r,echo=FALSE}

#Initials checks
summary(training)
unique(training$max_roll_belt)

#Flag fields which are almost entirely missing
missing_gt95_vec<-sapply(training,function(x) sum(is.na(x))/dim(training)[1]>0.95)
missing_gt95_names<-names(training)[missing_gt95_vec]


#Remove unwanted fields 
training<-training%>%select(-missing_gt95_names,"X")
testing<-testing%>%select(-missing_gt95_names,"X")
```


#Check the distribution of users is similar for train and test

Interesting to see that there is a higher proportion of in the test than the train, we could up wait this group when we come to modelling because for this project the marking is coming from a good prediction on the test, however I have decided not to do this because this would not normally be the correct way to use the test dataset.  

```{r}
table(training$user_name)/dim(training)[1]*100
table(testing$user_name)/dim(testing)[1]*100


```

# Create a train and test set from the training data

I have created a train and test data set as subsets of the training data this allows me to test the model performace on some out of sample data before I need to try it on the t

```{r}
intrain<-createDataPartition(y=training$classe,p=0.7,list=F)
train<-training[intrain,]
test<-training[-intrain,]
```


# Build models on the training data 

```{r}


# Set up 10 fold repeated cross validation to try and prevent overfitting when selecting the mtry parameter
control<-trainControl(method="repeatedcv",
                   number=10,
                   repeats=3,
                   search="grid")

# Set up number of predictors sampled as candiates for each split of the random forest - suggested default for classification is square root p where p is the number of predictors. 

tunegrid <- expand.grid(.mtry=c(5:15))

modellist <- list()

#Tune number of tree by looping through

#train with different ntree parameters
for (ntree in c(50,150,250,350)){
  set.seed(312)
  fit <- train(classe~.,
               data = train_samp,
               method = 'rf',
               metric = 'Accuracy',
               tuneGrid = tunegrid,
               trControl = control,
               ntree = ntree)
  key <- toString(ntree)
  modellist[[key]] <- fit
}

#Compare results
results <- resamples(modellist)
summary(results)



#Print out some model summary statistics on the train 
model_rf$finalModel$confusion
model_rf$results
plot(model_rf)

#Print prediction
pred_rf_inner_test<-predict(model_rf,newdata=test)
table(pred_rf_inner_test)


#Print prediction on test
pred_rf<-predict(model_rf,newdata=testing)
table(pred_rf)


```




```{r}
model_mn <-train(
  classe ~ .,
  data = training,
  method = "multinom",
  trControl = trainControl(method = "cv", number = 20),
  trace = FALSE,
  seed=312
)

pred_mn<-predict(model_mn,newdata=testing)
pred_mn<-predict(model_mn,newdata=training[samp2,])
table(training[samp2,]$classe)
table(pred_mn)
table(training[samp,]$classe)
```

```{r}

```


